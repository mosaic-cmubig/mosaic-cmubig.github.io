<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Project Page for MOSAIC Paper">
  <meta property="og:title" content="MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments"/>
  <meta property="og:description" content="MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments"/>
  <meta property="og:url" content="ariannaliu.github.io/MOSAIC/"/>
  <meta property="og:image" content="static/image/mosaic-icon.png" />
  <meta property="og:image:width" content="993"/>
  <meta property="og:image:height" content="819"/>

  <meta name="twitter:title" content="MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments">
  <meta name="twitter:description" content="Project Page for MOSAIC">
  <meta name="twitter:image" content="static/images/mosaic-icon.png">
  <meta name="twitter:card" content="MOSAIC">
  <meta name="keywords" content="Multi-view, Image geneation, consistent image, Privacy-Preserving, Depth conditioned generation, diffusion, computer vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments</title>
  <link rel="icon" type="image/x-icon" href="static/images/mosaic-icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <style>
    .highlight-box {
      background-color: #f5f5f5;
      padding: 20px;
      border-radius: 10px;
      margin-bottom: 20px;
      border-left: 5px solid #4682B4;
    }
    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      height: 0;
      overflow: hidden;
      max-width: 100%;
    }
    .video-container video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
    .key-finding {
      padding: 15px;
      margin-bottom: 15px;
      border-radius: 5px;
      background-color: #e6f2ff;
    }
    .task-section {
      margin-top: 30px;
      margin-bottom: 40px;
    }
    .section-title {
      border-bottom: 2px solid #4682B4;
      padding-bottom: 10px;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2rem;">MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ariannaliu.github.io/" target="_blank">Zhixuan Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://zwandering.github.io/" target="_blank">Haokun Zhu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://ruichen.pub/" target="_blank">Rui Chen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en" target="_blank">Jonathan Francis</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://soonminhwang.github.io/" target="_blank">Soonmin Hwang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://frc.ri.cmu.edu/~zhangji/" target="_blank">Ji Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">Jean Oh</a><sup>1</sup>
              </span>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>CMU  &nbsp <sup>2</sup>Bosch Center for AI  &nbsp  <sup>3</sup>Hanyang University</span>
                <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
              </div>

              

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Paper PDF link -->
                  <span class="link-block">
                    <a href="static/pdfs/paper.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <span class="link-block">
                    <a href="static/pdfs/appendix.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                    <!-- Github link -->

                    <!-- Coming soon link -->
                    <span class="link-block">
                    <a href="#" class="external-link button is-normal is-rounded is-dark" disabled>
                      <span class="icon">
                      <i class="fas fa-code"></i>
                      </span>
                      <span>Coming Soon</span>
                    </a>
                    </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="#" class="external-link button is-normal is-rounded is-dark" disabled>
                    <!-- <a href="https://arxiv.org/abs/2501.02630" target="_blank" class="external-link button is-normal is-rounded is-dark"> -->
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Overview video -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Overview</h2>
        <div class="video-container">
          <video autoplay controls muted loop>
            <source src="static/videos/supp_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="content has-text-centered mt-4">
          <p class="subsubtitle">MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce a novel diffusion-based approach for generating
              privacy-preserving digital twins of multi-room indoor environments from depth
              images only. Central to our approach is a novel <b>M</b>ulti-view <b>O</b>verlapped <b>S</b>cene
              <b>A</b>lignment with <b>I</b>mplicit <b>C</b>onsistency (<b>MOSAIC</b>) model that explicitly considers
              cross-view dependencies within the same scene in the probabilistic sense.
              MOSAIC operates through a novel inference-time optimization that avoids error
              accumulation common in sequential or single-room constraint in panorama-based
              approaches. MOSAIC scales to complex scenes with zero extra training and
              provably reduces the variance during denoising processes when more overlapping
              views are added, leading to improved generation quality. Experiments show that
              MOSAIC outperforms state-of-the-art baselines on image fidelity metrics in
              reconstructing complex multi-room environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Key Findings -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">Key Findings</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="key-finding">
            <h3 class="title is-4">Safety Through Compliance</h3>
            <p>MOE soft robot hand applies 74% less force than rigid grippers while grasping comparable amounts of hair, demonstrating that soft robots can provide more comfortable care without sacrificing effectiveness.</p>
          </div>
          
          <div class="key-finding">
            <h3 class="title is-4">Visual Force Estimation</h3>
            <p>Our novel force estimation module combines visual deformation data with tendon tensions to precisely track applied forces, reducing sensing errors by up to 60% compared to actuator-only approaches.</p>
          </div>
          
          <div class="key-finding">
            <h3 class="title is-4">User Preference</h3>
            <p>Our study with 12 participants showed statistically significant preference for MOE with force estimation across all tasks. Participants commented the system felt "really similar to human fingers" and provided a sensation "like a head massage."</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MOE Design -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">MOE: Multi-finger Omnidirectional End-effector</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <p>We introduce a dexterous tendon-driven soft robot manipulator that we call Multi-finger Omnidirectional End-effector (MOE) for hair-care applications.</p>
            
            <h3 class="title is-4">Design Features:</h3>
            <ul>
              <li><strong>Soft Fingers:</strong> Molded from low-hardness silicone (Ecoflex 00-30) for gentle contact</li>
              <li><strong>Tendon-Driven System:</strong> Two servomotors activate each finger through four embedded tendons</li>
              <li><strong>Wrist-Mounted Camera:</strong> RGBD camera provides egocentric view for force estimation</li>
              <li><strong>Human-Like Form Factor:</strong> 105mm length, 17mm diameter fingers designed for intuitive interaction</li>
            </ul>
            
            <div class="video-container mt-4 mb-4">
              <video autoplay controls muted loop>
                <source src="static/videos/video0.mp4" type="video/mp4">
              </video>
            </div>
            
            <p>The design addresses a key challenge in previous works where human subjects tend to perceive rigid robots as being "rough" during hair care tasks. MOE's compliance makes it safer in unstructured environments and more robust in contact-rich manipulation tasks.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Hair Care Tasks -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">Hair Care Skills</h2>
      
      <div class="task-section">
        <h3 class="title is-4">Head Patting</h3>
        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <p>MOE approaches the user's head from either the top or side to pat it, providing gentle contact with consistent force. The compliance of the fingers ensures safety during this interaction.</p>
              <p>This task demonstrates MOE's ability to make initial contact with the head in a controlled, comfortable manner.</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="task-section">
        <h3 class="title is-4">Finger Combing</h3>
        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <p>MOE follows a user-defined trajectory across the user's head for finger combing, maintaining consistent contact with the scalp even as it moves along a path.</p>
              <p>This task showcases the system's ability to maintain proper contact force during continuous movement, adapting to the contours of the head.</p>
            </div>
          </div>
        </div>
      </div>
      
      <div class="task-section">
        <h3 class="title is-4">Hair Grasping</h3>
        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <p>MOE approaches the user's head from either the top or side and grasps hair from near the scalp. The soft fingers allow for effective grasping while applying minimal force to the head.</p>
              <p>In our tests, MOE applied 74% less force than rigid grippers while still grasping a comparable amount of hair.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Force Estimation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">Force Estimation Module</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <p>To enable MOE to maintain contact with the head during tasks and perform tasks effectively and comfortably for users, we developed methods for predicting MOE contact forces using:</p>
            
            <ol>
              <li><strong>Visual Deformation:</strong> Using the wrist-mounted egocentric RGBD camera to capture depth images of MOE as it deforms during contact</li>
              <li><strong>Tendon Tension:</strong> Observing the actuator current load which correlates to tension on the tendons</li>
            </ol>
            
            <div class="columns mt-4">
              <div class="column">
                <div class="video-container">
                  <video autoplay controls muted loop>
                    <source src="static/videos/video4.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
              <div class="column">
                <div class="video-container">
                  <video autoplay controls muted loop>
                    <source src="static/videos/video5.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
            </div>
            
            <p class="mt-4">Our evaluation shows that the combined approach reduces sensing errors by up to 60.1% compared to actuator current load-only and 25.4% compared to depth image-only baselines.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- User Study Results -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">User Study</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <p>We conducted a user study with 12 participants with diverse hair types to evaluate MOE-Hair across three key dimensions:</p>
            
            <div class="columns">
              <div class="column">
                <div class="highlight-box">
                  <h4 class="title is-5">Task Effectiveness</h4>
                  <p>Participants rated MOE-Hair as significantly more effective than the vision-only baseline across all three tasks.</p>
                </div>
              </div>
              <div class="column">
                <div class="highlight-box">
                  <h4 class="title is-5">User Comfort</h4>
                  <p>Participants reported higher comfort levels, particularly in the more involved hair-grasping task.</p>
                </div>
              </div>
              <div class="column">
                <div class="highlight-box">
                  <h4 class="title is-5">Appropriate Use of Force</h4>
                  <p>Significant preference for MOE-Hair's force application, especially during finger combing.</p>
                </div>
              </div>
            </div>
            
            <div class="columns mt-4">
              <div class="column">
                <div class="video-container">
                  <video autoplay controls muted loop>
                    <source src="static/videos/video6_1.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
              <div class="column">
                <div class="video-container">
                  <video autoplay controls muted loop>
                    <source src="static/videos/video7.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
            </div>
            
            <div class="notification is-info is-light mt-4">
              <p class="has-text-weight-bold">Participant Feedback:</p>
              <p>"It felt really similar to human fingers... I kind of forgot it was a robot arm for a minute."</p>
              <p>"It was like a head massage."</p>
              <p>"I felt really comfortable."</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Future Work -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">Future Work & Extensions</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <p>We're extending MOE to various dexterity projects including:</p>
            
            <ul>
              <li>Dynamic pen spinning (<a href="https://arxiv.org/abs/2411.12734" target="_blank">arxiv.org/abs/2411.12734</a>)</li>
              <li>Learning in-hand manipulation from demonstration (<a href="https://arxiv.org/abs/2503.01078" target="_blank">arxiv.org/abs/2503.01078</a>)</li>
              <li>Additional hair care tasks including brushing and styling</li>
              <li>Applications for elderly and mobility-impaired users</li>
            </ul>
            
            <div class="video-container mt-4 mb-4">
              <video autoplay controls muted loop>
                <source src="static/videos/video8.mp4" type="video/mp4">
              </video>
            </div>
            
            <p>This work demonstrates the unique advantages of soft robots in contact-rich hair-care tasks, while highlighting the importance of precise force control despite the inherent compliance of the system.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Citation</h2>
      <div class="columns is-centered">
        <div class="column is-8">
          <pre><code>@inproceedings{yoo2025moehair,
  title={Soft and Compliant Contact-Rich Hair Manipulation and Care},
  author={Yoo, Uksang and Dennler, Nathaniel and Xing, Eliot and Mataric, Maja and Nikolaidis, Stefanos and Ichnowski, Jeffrey and Oh, Jean},
  booktitle={Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </section>

  <!-- Acknowledgements -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Acknowledgements</h2>
          <div class="content has-text-centered">
            <p>
              This work is supported by the NSF GRFP (Grant No. DGE2140739) and MOTIE, Korea (Grant No. 20018112).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          Website template based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </footer>

</body>
</html>